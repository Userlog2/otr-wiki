# Rating Calculation FAQ

o!TR primarily uses the [OpenSkill rating system](https://openskill.me/en/stable/index.html), which is based on [this paper](https://jmlr.csail.mit.edu/papers/volume12/weng11a/weng11a.pdf). In short, OpenSkill is a system similar to the Elo or Glicko/Glicko-2 rating systems used in games like chess - it assigns each player an approximate rating and rating deviation (with a higher rating deviation meaning more opportunity for the rating to increase or decrease), and after playing matches updates ratings based only on relative placements of the players in the match.

If you are interested in the **math details**, please scroll down to the bottom of the page.

## How are matches selected and filtered?
Tournaments are approved manually by a member of the o!TR team if they are fair and played in a head-to-head competitive environment. This means that most tournaments adhering to badging criteria are accepted, with qualifiers and tryouts excluded. Please see the [[tournaments|tournaments.md]] page of this wiki for more details.

Anyone is permitted to submit tournaments for approval, but we ask that a list of all bracket match MPs be provided for consistency. Matches also do some filtering for warmups (through uneven team sizes or unusual mods), but anyone is permitted to submit a case-by-case request to exclude a map or player.

## How are initial ratings determined? 
Everyone's starting rating is chosen in terms of your osu! rank using the **closest-known rank according to [osu!track](https://github.com/Ameobea/osutrack-api)** (or your most recent global rank if none is known). For a rough reference, you'll receive a typical starting rating if you're somewhere around the high 5 digit range (rank 10000). The initial placement is based on the closest point in time relative to when you started playing tournaments. For example, if a player's first match (in our verified match database) is from November 2021, and osu!track's closest record is from December 2021, we would base your initial rating on the December rank. Please remember that your rating will tend to approach your "true" rating after you have played in a few tournaments, no matter where you start.

## How are match scores interpreted?
OpenSkill updates ratings of players based only on relative ranking, and these relative rankings are decided using a match cost formula. Match cost is a measurement of how well a player does across the match (comparing their scores to the others in the lobby, plus giving a boost to players who are playing more frequently). This means players who perform better and play in the lobby more often (because they fill more skillsets in team tournaments) will be placed higher in the ranking and thus gain more rating. In other words, rating changes corresponds to **full matches** (in approved tournaments), not individual points. This mitigates circumstances where specialists have an inflated rating because they only play for the maps they play best.

Again, this is a key difference between o!TR and ETX/SIP: the latter two models consider more refined measurements of skill as well as the difficulty of the maps that are played. Since o!TR is primarily meant to measure head-to-head performance, it **does not take star rating into consideration**, and this means it is possible for low rank players to "farm rating" off of others by consistently performing well. This is intentional and should be viewed as a **sign the player may be sandbagging**. Additionally, it does not matter how much you win by as long as your match cost is higher than other players (this is also the same model that TETRIO uses). Again, all effects like this even out with more matches played, and dampening outliers is necessary for a volatile tournament system like osu!'s.

## When will ratings update?
Ratings will be recalculated **every Tuesday at 12 UTC**, taking into account any new matches and tournaments that have been submitted since the previous recalculation. Please note that when any tournament is added to the database, that can have a ripple effect and change ratings of not just the players in that tournament but anyone who has ever played them. Thus, **your rating may change even if you don't play any new matches**. This will remain true until we stop adding historical data, which could be years from now.

## How does rating decay work?
See below for more details, but every player is assigned a **rating** and **volatility**. Players start off with high volatility (meaning that they gain or lose rating more drastically at first), but volatility gradually decreases as a player competes in more tournaments and their rating settles. If a player does not play in any matches for around 4 months in a row, their volatility will begin to gradually increase over time (to indicate the model's uncertainty about your performance). Furthermore, players will start to lose a small amount of rating each week until they play again, down to "half of their peak performance." (Specifically, they decay down to **halfway between** their peak rating and a base value around 800 rating; this is similar to how Clash Royale and some MOBAs reset trophy counts and tiers after a season concludes.)

One common feedback o!TR has received from the community is that players may be incentivized to use rating decay or induce poor tournament performance to derank their o!TR, defeating the purpose of the system. However, the 4 month period is long enough and the rating decay is small enough that the effects would not be noticeable unless the players are trying to do something like only play the annual 5WC, at which point they would be isolating themselves from the rest of the tournament base. Furthermore, in any documented cases of rating sandbagging, the match can be manually flagged by o!TR to not count if foul play has been detected; this is a primary advantage of the tournament and match approval system in place.

---

# Math details and further explanation of the model

If you have further questions not answered on this page, feel free to ask in the [o!TR Discord](https://discord.gg/R53AwX2tJA) or start a GitHub discussion.

## Match cost
The match cost formula is inspired by Bathbot's formula, simplified to remove factors that are not relevant for our purposes. Each player receives a **map score** between 0.5 and 1.5 for each map they play, with 1 being average across the lobby (specifically, this is `0.5 + normal cdf(z-score)`). To calculate match cost, we take the average of these map scores and multiply by a **lobby bonus factor** ranging from 1 and 1.3 (specifically, this is `1 + 0.3 sqrt(x)`, where x ranges linearly from 0 for playing only one map to 1 for playing all of them). 

Bathbot also includes a bonus for playing in Tiebreaker and for playing more mod combinations, but both of those help reflect "general contribution to match" rather than "overall performance" which is why we do not use them.) Unlike other match cost formulas, this one ensures that **in a 1v1, the winner of the match should always have the higher match cost** unless warmups are mistakenly counted or the EZ multiplier is different from expected. (This is important because the winner of a 1v1 should always gain rating.)

## Rating change formulas
The rating system itself is based on OpenSkill, which is a **Bayesian approximation algorithm**. Without going too deep into any formulas (that's what reading the paper is for), these Bayesian rating algorithms assign each player a rating μ and volatility σ, which together describe a distribution of predicted actual skill levels for that player. When players compete against each other in a match, a formula is used to calculate the probability of various outcomes / rankings of that match, and then all of the players' μ and σ values are adjusted based on how "surprising" the match outcome was. In this case, the formula comes from the Plackett-Luce model, whose fundamental assumption is **irrelevance of alternatives**. Specifically, Plackett-Luce is based on the idea that player A beats player B in match cost with the same probability, no matter who else is in the lobby. This is not exactly correct because other teammates may affect how often you're playing, but it is a model used in real-world ranking systems like horse racing or poker standings.

Quoting from the paper, these adjustments are calculated via "the average of the relative rate of change of (a player's) winning probability with respect to (their) strength*," where the average is taken over the prior distribution. The actual Bayesian inference calculations for these types of models are computationally intensive, and OpenSkill is actually only an **approximation** of the full calculation (in the paper, they discuss why the simplifications are reasonable). Thus, we use OpenSkill because (1) it has an open license and (2) every time new matches are added, the **entire rating history** of all matches must be recalculated and thus a faster algorithm is favorable.

## Choosing parameters
The Plackett-Luce model allows for arbitrary scalings of parameters, though the OpenSkill documentation recommends that σ and β start out as 1/3 and 1/6 of μ. We choose a scaling here so that the highest ratings look somewhat similar to chess (solely for aesthetic appeal), though we do choose varying initial ratings based on rank and also currently initialize σ and β to a smaller fraction of μ instead of the default to make it more difficult to harder to farm rating off low-rating players. Remember that different players specialize in different skillsets and have skillcaps at different levels, so **please interpret o!TR not as an absolute skill comparison between two players**. (If you are interested in figuring out matchup win probabilities, the Skill Issue bot addresses those questions more directly.) Again, remember that o!TR identifies when people **frequently win** relative to others in their rank range or skill level, so if you see a player with what seems like an unusually high rating, we recommend that you look at their tournament history and check if they're consistently the top match cost in their matches.